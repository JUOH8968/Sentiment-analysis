# -*- coding: utf-8 -*-
"""국비교육_감정분석.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LRGd8xwZlwVkMbcqC8QfutHu_axUpN65

# 미니프로젝트_감정분석

## 1. 웹 리뷰크롤링
"""

pip install google_play_scraper

pip install tika

pip install konlpy

## google_play_scraper 에 속해있는 함수확인하기
import google_play_scraper
dir(google_play_scraper)

#  함수가 필요로 하는 인자값 확인
help(reviews_all)

import pandas as pd
import numpy as np
from google_play_scraper import app, Sort, reviews_all

url = "com.fineapp.yogiyo"
# com.coupang.mobile.eats-> 쿠팡이츠
# com.sampleapp -> 배달의 민족
# com.fineapp.yogiyo --> 요기요

# 앱 정보 크롤링
app = app(url,lang = "ko", #언어 한국어
                country = "kr") #나라 한국으로 설정

# 앱 리뷰 크롤링
review = reviews_all(url,
                    sleep_milliseconds = 500, # 0.5초 대기시간
                    lang = "ko",# default = "en"
                    country = "kr", # default = "US"
                    sort = Sort.MOST_RELEVANT, # Sort.NEWEST , 관련성 높은순으로 정렬
                    filter_score_with = None # None means All score
                    )
# 데이터프레임 변환
df_yo= pd.DataFrame(review)
df_yo.head(2)

import pandas as pd
import numpy as np
from google_play_scraper import app, Sort, reviews_all

url = "com.coupang.mobile.eats"
# com.coupang.mobile.eats-> 쿠팡이츠
# com.sampleapp -> 배달의 민족
# com.fineapp.yogiyo --> 요기요

# 앱 정보 크롤링
app = app(url,lang = "ko", #언어 한국어
                country = "kr") #나라 한국으로 설정

# 앱 리뷰 크롤링
review = reviews_all(url,
                    sleep_milliseconds = 500, # 0.5초 대기시간
                    lang = "ko",# default = "en"
                    country = "kr", # default = "US"
                    sort = Sort.MOST_RELEVANT, # Sort.NEWEST , 관련성 높은순으로 정렬
                    filter_score_with = None # None means All score
                    )
# 데이터프레임 변환
df_co = pd.DataFrame(review)
df_co.head(2)

import pandas as pd
import numpy as np
from google_play_scraper import app, Sort, reviews_all

url = "com.coupang.mobile.eats"

# 앱 정보 크롤링
app = app(url,lang = "ko", #언어 한국어
                country = "kr") #나라 한국으로 설정

# 앱 리뷰 크롤링
review = reviews_all(url,
                    sleep_milliseconds = 500, # 0.5초 대기시간
                    lang = "ko",# default = "en"
                    country = "kr", # default = "US"
                    sort = Sort.MOST_RELEVANT, # Sort.NEWEST , 관련성 높은순으로 정렬
                    filter_score_with = None # None means All score
                    )
# 데이터프레임 변환
df_review = pd.DataFrame(review)
df_review.head(2)

df= pd.concat([df_co,df_yo])
df

df = df[['content','score']]
# df=DataFrame(df_review,columns=['content','score']) #위에 데이터프레임에서 content와 score 만 추출
df

df.sort_values(['score'], ascending=[False]) #내림차순으로 정렬

worst= df[:][df['score']==1] # score 값이 1점인 리뷰만 추출
worst

best= df[:][df['score']==5] # score 값이 1점인 리뷰만 추출
best

worst_all= df[:] [(df['score']==1) | (df['score']==2)]
worst_all

# txt 파일로 저장
worst_all.to_csv('worst_all.txt', index=False, header=None, sep="\t")
best.to_csv('best.txt', index=False, header=None, sep="\t")

"""## 2. 데이터 전처리

### 1. 텍스트 불러오기
"""

import pickle
from konlpy.tag import Okt
import urllib.request
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences


# 별점 5점인 데이터에서 라벨 1 추가
df_best = pd.read_table('best.txt', names=['content','label'])
df_best['label']= 1
df_best

# 별점 1점과 0점인 데이터에서 라벨 0 추가
df_worst = pd.read_table('worst_all.txt', names=['content','label'])
df_worst['label']=0
df_worst

"""### 2. 각 라벨값이 유일한지 확인"""

print(df_best['content'].nunique(), df_best['label'].nunique())
print(df_worst['content'].nunique(), df_worst['label'].nunique())

"""### 3. 중복 제거"""

df_best.drop_duplicates(subset=['content'], inplace=True)
df_worst.drop_duplicates(subset=['content'], inplace=True)

print(len(df_best))
print(len(df_worst))

"""### 4. 결측치 제거"""

df_best_d = df_best.dropna()
print(len(df_best_d))

df_worst_d = df_worst.dropna()
print(len(df_worst_d))

"""### 5. 데이터 정규화"""

# 데이터 정규화
df_best_d['content'] = df_best_d['content'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
df_worst_d['content'] = df_worst_d['content'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")

# df_best_d['content'] = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]',"",df_best_d['content']) 에러로 replace 사용
# df_best_d['content'] 가 여러문자를 담고있어서 에러남

"""## 3. 명사추출, 동사추출
### https://limsw.tistory.com/72
### https://soyoung-new-challenge.tistory.com/31
"""

df_worst_d['content'].to_csv('worst.txt' ,index=False, header=None, sep="\t")

df_best_d['content'].to_csv('new_best.txt' ,index=False, header=None, sep="\t")

# 정규화한 df_worst_d에서 txt 파일로 저장하여 동의어 찾기
# df_worst_d['content'].to_csv('worst.txt' ,index=False, header=None, sep="\t")

# 명사 추출
from konlpy.tag import Okt
from collections import Counter

okt = Okt()
f = open('worst.txt', 'r', encoding='utf-8')
sentences = f.read()

noun = okt.nouns(sentences)
counter = Counter(noun)

top_100_noun_list = counter.most_common(100)

for i in top_100_noun_list:
    print(i)


## 동사추출
# f = open('worst.txt', 'r', encoding='utf-8')
# sentences = f.read()

# okt = Okt()

# # 형태소 분석 (품사 태깅)
# pos_tags = okt.pos(sentences,stem=True)

# # 동사만 추출
# verbs = [word for word, pos in pos_tags if pos == 'Verb']
# counter = Counter(verbs)
# top_100_verbs_list = counter.most_common(100)
# for i in top_100_verbs_list:
#   print(i)

"""## 4. 데이터증강
### https://fish-tank.tistory.com/95
### https://working-helen.tistory.com/50
"""

import pandas as pd
import random
from konlpy.tag import Okt
import numpy as np

# 형태소 분석기
okt = Okt()

# 동의어 사전
# Word2Vec / fastText 같은 임베딩 기반으로 자동 생성하기
# 특정 단어를 발견하면 동의어 중 하나로 바꿔줌
synonym_dict = {
    "나쁘다": ["별로다", "형편없다", "불만족스럽다"],
    "느리다": ["답답하다", "굼뜨다","느지막하다","느직느직하다",'늦다'],
    "불편하다" : ['갑갑하다','거북하다','괴롭다'],
    "힘들다" : ['거칠다','고단하다','어렵다'],
    "짜증" : ['신경질','역정','역증'],
    "어이없다" : ['어처구니없다','기가막히다'],
    "비싸다" : ['값이높다','값비싸다'],
    "아쉽다": ['안타깝다','서운하다'],
    "최악" : ['가장나쁨'],
    "빡치다": ['화나다','열받는다'],
    "실망스럽다" : ['낙심하다','비관하다','상실하다'],
    "왜": ['어째서','어찌','어째'],
    "오류" : ['과실','실수'],
    "문제" : ['말썽거리','말썽'],
    "정말" : ['사실'],
    "기다리다" : ['고대하다','기대하다','거종하다'],
    "모르다" : ['미지하다','부지하다'],
    "사라지다" : ['날아가다'],
    "취소" : ['결렬','철회','무산']

}

# 동의어 치환
# 한 문장에서 n개의 단어를 동의어로 치환
def synonym_replacement(sentence, n=2):
    words = okt.morphs(sentence)
    new_words = words.copy() # 원본보존
    candidates = [w for w in words if w in synonym_dict] # 사전에 있는 단어만 후보로
    random.shuffle(candidates)

    for i in range(min(n, len(candidates))):
        word = candidates[i]
        synonym = random.choice(synonym_dict[word])
        new_words = [synonym if w == word else w for w in new_words]
    return ''.join(new_words)

# 랜덤삭제
# 확률 p로 각 단어를 랜덤삭제
def random_deletion(sentence, p=0.2):
    words = okt.morphs(sentence)
    if len(words) == 1:
        return sentence
    new_words = [w for w in words if random.uniform(0,1) > p]
    if len(new_words) == 0:
        return random.choice(words)
    return ''.join(new_words)

# EDA 함수
def eda(sentence, num_aug):
    augmented = []
    augmented.append(synonym_replacement(sentence))
    augmented.append(random_deletion(sentence))
    return augmented[:num_aug] # num_aug의 수만큼 잘라서 반환

# 데이터 증강
def augment_dataframe(df_worst, target_size):
    augmented_texts = []
    original_texts = df_worst['content'].tolist() # tolist() : 같은 위치에 있는 데이터끼리 묶어준다

    while len(original_texts) + len(augmented_texts) < target_size:
        sentence = random.choice(original_texts)  # 원본에서 랜덤 문장 선택
        augmented_sentences = eda(sentence, num_aug=2) # EDA 적용
        augmented_texts.extend(augmented_sentences) # 증강된 문장 추가, augmented_sentences 이것이 리스트여서 extend 사용

    # 필요한 개수만큼 자르기
    augmented_texts = augmented_texts[:target_size - len(original_texts)]

    # 원본 + 증강 데이터 합치기
    final_texts = original_texts + augmented_texts
    df_augmented = pd.DataFrame(final_texts, columns=['content'])
    return df_augmented

df_best_d_size = len(df_best_d)
df_worst_d_size = len(df_worst_d)

df_worst_d_augmented = augment_dataframe(df_worst_d, df_best_d_size)

### 확인
print("원본 worst 데이터 개수:", df_worst_d_size)
print("증강 후 worst 데이터 개수:", len(df_worst_d_augmented))

## 라벨링
df_worst_d_augmented['label']=0
# df_worst_d_augmented.columns=['content','label']
df_worst_d_augmented

df_worst_d_augmented.to_csv('worst_aug.txt', index=False, header=None, sep="\t")

"""## 5. 긍정어와 부정어 워드클라우드
### https://coding-yesung.tistory.com/201
### https://chongmin-k.tistory.com/entry/%EC%9B%8C%EB%93%9C%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%98%A4%EB%A5%98-cannot-open-resource
"""

### 주피터 노트북에서 실행
# !pip install wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt

font_path =  'C:/Users/HJO/Desktop/python/malgun.ttf' #'C:/Windows/Fonts/malgun.ttf'

text = open('worst_aug.txt', encoding='utf-8').read()

# font_path 매개변수에 한글 폰트 경로 지정
wordcloud = WordCloud(
    font_path=font_path,
    width=800,
    height=800,
    background_color='white',
    stopwords= #불용어 지정 변수).generate(text)
).generate(text)

plt.figure(figsize=(10, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""### 6. 훈련데이터 , 테스트 데이터 split"""

from sklearn.model_selection import train_test_split
train_data_b, test_data_b = train_test_split(df_best_d, test_size=0.3, random_state=123)

print("Train data size:", len(train_data_b))
print("Test data size:", len(test_data_b))

train_data_w, test_data_w = train_test_split(df_worst_d_augmented, test_size=0.3, random_state=123)

print("Train data size:", len(train_data_w))
print("Test data size:", len(test_data_w))

# train 데이터끼리 묶기
train_data = pd.concat([train_data_b, train_data_w])
train_data

# test 데이터끼리 묶기
test_data = pd.concat([test_data_b, test_data_w])
test_data

print(test_data['content'].nunique(), test_data['label'].nunique())
print(train_data['content'].nunique(), train_data['label'].nunique())

train_data['label'].value_counts().plot(kind = 'bar')

"""### 8. 토큰화"""

import pickle
import pandas as pd
import numpy as np
import re
from konlpy.tag import Okt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences


# 불용어 지정
stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자',
             '에','와','한','하다']

# okt를 활용한 형테소 분석
okt = Okt()
x_train = []
for sentence in train_data['content']:
    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화
    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
    x_train.append(stopwords_removed_sentence)

print(x_train[:2])

x_test = []
for sentence in test_data['content']:
    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화
    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
    x_test.append(stopwords_removed_sentence)

print(x_test[:2])

"""### 9. 정수 인코딩 : 훈련 데이터에 대해 단어집합(vacaburary) 만듦"""

## 단어에 숫자지정
tokenizer = Tokenizer()
tokenizer.fit_on_texts(x_train)

print(tokenizer.word_index)

word_size = len(tokenizer.word_index)
print('word_size : ', word_size)

"""### 12. 텍스트 시퀀스를 정수 시퀀스로 변환"""

# tokenizer = Tokenizer(vocab_size)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(x_train)
x_train = tokenizer.texts_to_sequences(x_train)
x_test = tokenizer.texts_to_sequences(x_test)

print(x_train[:3])

y_train = np.array(train_data['label'])
y_test = np.array(test_data['label'])

# X_train = np.delete(X_train, drop_train, axis=0. dtype=object)
# X_train = np.delete(x_train, drop_train, axis=0)
# y_train = np.delete(y_train, drop_train, axis=0)
print(len(x_train))
print(len(y_train))

"""### 14. 패딩 : 서로 다른 길이의 샘플들의 길이를 동일하게 맞춰주는 패딩 작업을 진행"""

# 단어 인덱스로 바꾼 뒤 길이가 다 다르기 때문에 모두 동일한 길이로 맞춰줘야 함
# y_train은 단일값(0,1)로 이루어져 있어서 패딩 안해도 됨
x_train = pad_sequences(x_train, maxlen=100) # maxlen : 문장의 길이를 보고 결정
x_test = pad_sequences(x_test, maxlen=100)

x_train = np.array(x_train)
y_train = np.array(y_train)

"""## 3. 감정분석 모델링
### https://dev-with-gpt.tistory.com/55
"""

from tensorflow.keras.layers import Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

embedding_dim = 100
hidden_units = 128

model = Sequential()
model.add(Embedding(word_size+1, embedding_dim)) # word_size: 37820, 임베딩과정
model.add(LSTM(hidden_units))
model.add(Dense(1, activation='sigmoid'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.fit(x_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)

"""### 1. 훈련 과정에서 검증 데이터의 정확도가 가장 높았을 때 저장된 모델인 'best_model.h5'를 로드"""

loaded_model = load_model('best_model.h5')
print("\n 테스트 정확도: %.4f" % (loaded_model.evaluate(x_test, y_test)[1]))

def sentiment_predict(new_sentence):
  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)
  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화
  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거
  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩
  pad_new = pad_sequences(encoded, maxlen = 100) # 패딩
  score = float(loaded_model.predict(pad_new)) # 예측
  if(score > 0.5):
    print("{:.2f}% 확률로 긍정 리뷰입니다.\n".format(score * 100))
  else:
    print("{:.2f}% 확률로 부정 리뷰입니다.\n".format((1 - score) * 100))

sentiment_predict('너무 불편해요. 지역을 입력하고 주변 매장을 확인할 때 목록으로만 확인해야해서 매장 선택해서 가게 위치 하나하나 확인했네요. 다른 어플처럼 목록으로뿐 아니라 지도상으로도 주변 매장 위치들이 한눈에 보이면 좋을텐데요')

"""### 2. 모델저장"""

with open('tokenizer.pickle', 'wb') as handle:
     pickle.dump(tokenizer, handle)

#with open('tokenizer.pickle', 'rb') as handle:
#    tokenizer = pickle.load(handle)

"""### 3. 예측하기 : 임의의 리뷰에 대해서 예측하는 함수"""

def sentiment_predict(new_sentence):
  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)
  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화
  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거
  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩
  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩
  score = float(loaded_model.predict(pad_new)) # 예측
  if(score > 0.5):
    print("{:.2f}% 확률로 긍정 리뷰입니다.\n".format(score * 100))
  else:
    print("{:.2f}% 확률로 부정 리뷰입니다.\n".format((1 - score) * 100))

sentiment_predict('너무 불편해요. 지역을 입력하고 주변 매장을 확인할 때 목록으로만 확인해야해서 매장 선택해서 가게 위치 하나하나 확인했네요. 다른 어플처럼 목록으로뿐 아니라 지도상으로도 주변 매장 위치들이 한눈에 보이면 좋을텐데요')

sentiment_predict('위메프가 뭘 하겟냐만은. 설치도 안돼는 꼬락서니하곤.배송늦고 쓰레기만팔고 환불도 안해줘 진작 끊엇는데 배달도 그렇겟지 쓰레기로 갖다주고 늦게 오겟지.애국심에 설치하렷더만.ㅉ모든프로그램개발사를 제대로 선정못하네.비리가 잇것지.인터넷장사하는데 프로그램이 이상해서야.')

sentiment_predict('생각보다 너무 깔끔하고 보기 편하게 되어있어서 쓰기 편하고 공공배달서비스라서 쓰고 있습니당')

